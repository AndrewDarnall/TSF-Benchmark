# Liquid Time Constant Networks Presentation

I had the pleasure of presenting the slides above as a seminar to the 
**First Person Vision Group** at the 
**Department of Mathematics and Computer Science** of the 
**University of Catania**. 
It was an honor to work with this group under a research scholarship, focusing on topics such as **Liquid Time Constant Networks**, **Closed Form Continuous Time Networks**, and **Neural Circuit Policies**.

In the seminar, I provided an overview of **differential equations** and **dynamical systems**, emphasizing their significance in scientific research and discussing methods for computing the integral of differential equations.

I then introduced **Continuous Deep Learning models**, comparing them with traditional Deep Learning models and highlighting the differences in their mathematical foundations.

Next, I presented **Neural ODEs**, which marked an important step in the advancement of Continuous Deep Models. Following this, I introduced **Liquid Time Constant Networks**, an innovative deep learning technology developed by Dr. Ramin Hasani at MIT's Computer Science AI Laboratory. I discussed their relationship with **Dynamic Causal Modeling** and **Neuro-Plasticity**.

I reviewed some findings from the original papers, before introducing the closed-form approximation of the system of differential equations that govern Liquid Time Constant Networks.

Finally, I presented the **Neural Circuit Policy** architecture, which has been successfully deployed for an autonomous driving task focused on **lane keeping**.

However, it is important to note that this presentation is not exhaustive; it merely represents a portion of my research during my time with the **FPV Lab**.

